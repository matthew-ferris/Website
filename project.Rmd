---
title: "Using Machine Learning Classification Techniques to Predict Going Concern Opinions"
author: "Matt Ferris"
date: "November 26, 2021"
output: 
  distill::distill_article
---

```{r Load Libraries and stuffs,include=FALSE,echo=FALSE,message=FALSE,warning=FALSE}
library(lattice)
library(ggplot2)
library(caret)
library(tidyverse)
library(corrplot)
library(stringr)
library(RANN)
library(xaringan)
library(xaringanExtra)
library(VIM)

xaringanExtra::use_panelset()
xaringanExtra::style_panelset_tabs()

draw_confusion_matrix <- function(cm) {
 
  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)
 
  # create the matrix
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 435, 'Class1', cex=1.2)
  rect(250, 430, 340, 370, col='#F7AD50')
  text(295, 435, 'Class2', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#F7AD50')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, 'Class1', cex=1.2, srt=90)
  text(140, 335, 'Class2', cex=1.2, srt=90)
 
  # add in the cm results
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')
 
  # add in the specifics
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)
 
  # add in the accuracy information
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
} 
```

```{r Load Data, message=FALSE, include=FALSE, warning=FALSE,echo=FALSE}
setwd("~/Desktop/Analytics/Website")

Data<-read.csv("Data.csv",na.strings = ".")
attach(Data)
Data$GOING_CONCERN<-as.factor(Data$GOING_CONCERN)
Data$GOING_CONCERN<-ifelse(Data$GOING_CONCERN=="1","Yes","No")

```

## Purpose

Using data obtained through Dr. France at Mississippi State University, I plan on using machine learning to classify the likelihood that a Going Concern opinion will be issued by an auditor. The data has observations for the outcomes of audits for companies, the financial statement statistics from the companies, and the audit fees. The data include attributes such as current and prior year cash flows, revenues, net income, shares outstanding, total assets, company book value, and total audit fees.  

## Getting Started & Descriptive Statistics

To get started, I wanted to see the distribution of the dependent variable, which would be the Going Concern attribute. 


<aside>
```{r Dependent Variable Distribution Pt. 1, echo=TRUE}
table(Data$GOING_CONCERN)
```
</aside>  


You can see that the distribution is very heavily skewed. Of the nearly 56,000 observations, just under 6,500 of them issued a Going Concern opinion. 

After reviewing the dataset some more, I noticed that there were a relatively large amount of NAs in the observations. Using this formula, you can see the percentage of the data that is NA. 

<aside>
```{r How much NA?, echo=TRUE}
sum(is.na(Data))/prod(dim(Data))
```
<aside>

Roughly 10.5% of the cells are NA. There are a few ways to continue, such as different imputation techniques to make "estimates" of what those empty cells should be. However, for the sake of not murdering my computer, I have elected to simply omit NAs from the data. 

<aside>
```{r Remove NAs, echo=TRUE}
Data<-na.omit(Data)
```
<aside>

After omitting the NAs, I wanted to see the new distribution of the dependent variable. 

<aside>
```{r Dependent Variable Distribution Pt. 2, echo=TRUE}
table(Data$GOING_CONCERN)
```
<aside>

The dataset has now been reduced from 56,000 observations to 40,500. Of those entries removed, a large percentage of them were those that had Going Concern opinions.

## Making Predictions

To use machine learning to make the prediction of whether a Going Concern will be issued, I will use K-Nearest Neighbors classification. In short, K-NN attempts to use independent variables to make a prediction of what the dependent variable will be. In this case, the algorithm will predict whether a Going Concern opinion was issued by the company's auditor. The algorithm does this by measuring the distance between the "test data," which is what we are trying to predict, and the "training data." The model then makes a prediction based on the nearest data points in the training data. 

First, I will need to partition the data into two different sets: the first as a "training" set, which 'trains' the algorithm, and the second as a "testing" set, which will be used to test the fit of the model. Using the KNN method to make the predictions, I will then visualize the results in a confusion matrix. 

Due to the data being unbalanced, I have run two models with KNN. The first is the standard, while in the second I implement SMOTE sampling. You can click through the two tabs below to see the code for each of the models.

::::: {.panelset}

::: {.panel}
[Regular]{.panel-name}

```{r Knn Classification pt 1,message=FALSE,warning=FALSE}
#split the test/train data
trainIndex <- createDataPartition(y = Data[,names(Data) == "GOING_CONCERN"], p = .6, list = FALSE, times = 1)
#grab the data
DataTrain <- Data[ trainIndex,]
DataTest  <- Data[-trainIndex,]

#train the model 
KNN<-train(GOING_CONCERN~MATCHFY_BALSH_ASSETS+MATCHFY_BALSH_BOOK_VAL+MATCHFY_CSHFLST_CHANGE_TTM+MATCHFY_CSHFLST_OP_ACT_TTM+MATCHFY_INCMST_NETINC_TTM+PRIORFY_INCMST_NETINC_TTM+PRIORFY_BALSH_BOOK_VAL+MATCHFY_SUM_AUDFEES+PRIORFY_SUM_AUDFEES+CIK_Code,
               data=DataTrain,
               method="knn")

#make predictions of the test data based off of the model 
knn_pred<-predict(KNN,DataTest)

#compare the predictions to the actual
ConfusionMatrix1<-confusionMatrix(knn_pred,as.factor(DataTest$GOING_CONCERN))
```

:::

::: {.panel}
[SMOTE Sampling]{.panel-name}

```{r Knn Classification pt 2 SMOTE,message=FALSE,warning=FALSE}
#train the model 
KNN_SM<-train(GOING_CONCERN~MATCHFY_BALSH_ASSETS+MATCHFY_BALSH_BOOK_VAL+MATCHFY_CSHFLST_CHANGE_TTM+MATCHFY_CSHFLST_OP_ACT_TTM+MATCHFY_INCMST_NETINC_TTM+PRIORFY_INCMST_NETINC_TTM+PRIORFY_BALSH_BOOK_VAL+MATCHFY_SUM_AUDFEES+PRIORFY_SUM_AUDFEES+CIK_Code,
               data=DataTrain,
               trControl = trainControl(method = "cv",
                           number = 10,
                           classProbs = TRUE,
                           sampling = "smote"),
               method="knn")

#make predictions of the test data based off of the model 
knn_pred_sm<-predict(KNN_SM,DataTest)

#compare the predictions to the actual
ConfusionMatrix2<-confusionMatrix(knn_pred_sm,as.factor(DataTest$GOING_CONCERN))
```


:::

:::::

## The Results

The table below displays what the model predicted vs. what actual occurred. Class2 indicates a Going Concern opinion issued, whereas Class1 is the opposite. 

::::: {.panelset}

::: {.panel}
[Regular]{.panel-name}

``` {r CM1, echo = FALSE}
CM1<-draw_confusion_matrix(ConfusionMatrix1)

CM1
```

:::

::: {.panel}
[SMOTE]{.panel-name}

```{r CM2, echo = FALSE}
CM2<-draw_confusion_matrix(ConfusionMatrix2)

CM2
```

:::

:::::

The accuracy, precision, and recall statistics for the first model are *spectacular*. However, the specificity statistic is not so great. The Confusion Matrix shows that a rather large number of **actual** Going Concern companies were predicted to be fine. 

Using SMOTE sampling, the sensitivity, recall, and kappa statistics dropped by a relatively insignificant amount. The specificity statistic is much higher, though. The Confusion Matrix for this model shows that a much higher amount of the **actual** Going Concern companies were predicted by the model. However, the model also predicted a much higher amount of non-Going Concern companies to have that issue.

## How can this be useful? 

This model could be used to help identify those companies that are more likely to have a Going Concern issue. This model could help to reduce the likelihood of a type 2 error, which is when the auditor falsely accepts the financial statements, when they should have issued a Going Concern opinion. Using the SMOTE model, an auditor can identify those companies that have a much higher likelihood of having a Going Concern issue. However, there could be a lack of efficiency due to the model identifying a decent amount of non-Going Concern companies. 

